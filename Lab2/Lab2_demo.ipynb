{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据目录\n",
    "data_path = './data/tiny-imagenet-200/'\n",
    "\n",
    "\n",
    "# 定义数据类处理文件\n",
    "class RawData:\n",
    "\n",
    "    __labels_t_path = '%s%s' % (data_path, 'wnids.txt')\n",
    "    __train_data_path = '%s%s' % (data_path, 'train/')\n",
    "    __val_data_path = '%s%s' % (data_path, 'val/')\n",
    "\n",
    "    __labels_t = None\n",
    "    __image_names = None\n",
    "\n",
    "    __val_labels_t = None\n",
    "    __val_labels = None\n",
    "    __val_names = None\n",
    "\n",
    "    @staticmethod\n",
    "    def labels_t():\n",
    "        if RawData.__labels_t is None:\n",
    "            labels_t = []\n",
    "            with open(RawData.__labels_t_path) as wnid:\n",
    "                for line in wnid:\n",
    "                    labels_t.append(line.strip('\\n'))\n",
    "\n",
    "            RawData.__labels_t = labels_t\n",
    "\n",
    "        return RawData.__labels_t\n",
    "\n",
    "    @staticmethod\n",
    "    def image_names():\n",
    "        if RawData.__image_names is None:\n",
    "            image_names = []\n",
    "            labels_t = RawData.labels_t()\n",
    "            for label in labels_t:\n",
    "                txt_path = RawData.__train_data_path + label + '/' + label + '_boxes.txt'\n",
    "                image_name = []\n",
    "                with open(txt_path) as txt:\n",
    "                    for line in txt:\n",
    "                        image_name.append(line.strip('\\n').split('\\t')[0])\n",
    "                image_names.append(image_name)\n",
    "\n",
    "            RawData.__image_names = image_names\n",
    "\n",
    "        return RawData.__image_names\n",
    "\n",
    "    @staticmethod\n",
    "    def val_labels_t():\n",
    "        if RawData.__val_labels_t is None:\n",
    "            val_labels_t = []\n",
    "            with open(RawData.__val_data_path + 'val_annotations.txt') as txt:\n",
    "                for line in txt:\n",
    "                    val_labels_t.append(line.strip('\\n').split('\\t')[1])\n",
    "\n",
    "            RawData.__val_labels_t = val_labels_t\n",
    "\n",
    "        return RawData.__val_labels_t\n",
    "\n",
    "    @staticmethod\n",
    "    def val_names():\n",
    "        if RawData.__val_names is None:\n",
    "            val_names = []\n",
    "            with open(RawData.__val_data_path + 'val_annotations.txt') as txt:\n",
    "                for line in txt:\n",
    "                    val_names.append(line.strip('\\n').split('\\t')[0])\n",
    "\n",
    "            RawData.__val_names = val_names\n",
    "\n",
    "        return RawData.__val_names\n",
    "\n",
    "    @staticmethod\n",
    "    def val_labels():\n",
    "        if RawData.__val_labels is None:\n",
    "            val_labels = []\n",
    "            val_labels_t = RawData.val_labels_t()\n",
    "            labels_t = RawData.labels_t()\n",
    "            for i in range(len(val_labels_t)):\n",
    "                for i_t in range(len(labels_t)):\n",
    "                    if val_labels_t[i] == labels_t[i_t]:\n",
    "                        val_labels.append(i_t)\n",
    "            val_labels = np.array(val_labels)\n",
    "\n",
    "            RawData.__val_labels = val_labels\n",
    "\n",
    "        return RawData.__val_labels\n",
    "\n",
    "\n",
    "# 定义 Dataset 类\n",
    "class Data(Dataset):\n",
    "\n",
    "    def __init__(self, type_, transform):\n",
    "        \"\"\"\n",
    "        type_: 选择训练集还是验证集\n",
    "        \"\"\"\n",
    "        self.__train_data_path = '%s%s' % (data_path, 'train/')\n",
    "        self.__val_data_path = '%s%s' % (data_path, 'val/')\n",
    "\n",
    "        self.type = type_\n",
    "\n",
    "        self.labels_t = RawData.labels_t()\n",
    "        self.image_names = RawData.image_names()\n",
    "        self.val_names = RawData.val_names()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = None\n",
    "        image = None\n",
    "\n",
    "        labels_t = self.labels_t\n",
    "        image_names = self.image_names\n",
    "        val_labels = RawData.val_labels()\n",
    "        val_names = self.val_names\n",
    "\n",
    "        if self.type == \"train\":\n",
    "            label = index // 500  # 每个类别的图片 500 张\n",
    "            remain = index % 500\n",
    "            image_path = os.path.join(self.__train_data_path, labels_t[label], 'images', image_names[label][remain])\n",
    "            image = cv2.imread(image_path)\n",
    "            image = np.array(image).reshape(64, 64, 3)\n",
    "\n",
    "        elif self.type == \"val\":\n",
    "            label = val_labels[index]\n",
    "            val_image_path = os.path.join(self.__val_data_path, 'images', val_names[index])\n",
    "            image = np.array(cv2.imread(val_image_path)).reshape(64, 64, 3)\n",
    "\n",
    "        return label, self.transform(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        len_ = 0\n",
    "        if self.type == \"train\":\n",
    "            len_ = len(self.image_names) * len(self.image_names[0])\n",
    "        elif self.type == \"val\":\n",
    "            len_ = len(self.val_names)\n",
    "\n",
    "        return len_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    \"\"\"残差网络\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, dropout=False):\n",
    "        super(residual_block, self).__init__()\n",
    "\n",
    "        self.cov1 = nn.Conv2d(channel, channel, 3, 1, 1)\n",
    "        self.cov2 = nn.Conv2d(channel, channel, 3, 1, 1)\n",
    "\n",
    "        self.nor1 = nn.BatchNorm2d(channel)\n",
    "        self.nor2 = nn.BatchNorm2d(channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = self.cov1(x)\n",
    "        x_ = self.nor1(x_)\n",
    "        x_ = self.relu(x_)\n",
    "\n",
    "        x_ = self.cov2(x_)\n",
    "        x_ = self.nor2(x_)\n",
    "        x_ = self.relu(x_)\n",
    "\n",
    "        if self.dropout:\n",
    "            x_ = self.drop(x_)\n",
    "\n",
    "        x = x + x_\n",
    "        return x\n",
    "    \n",
    "\n",
    "class nor_cov(nn.Module):\n",
    "    \"\"\"单层卷积网络\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, dropout=False, normalize=False):\n",
    "        super(nor_cov, self).__init__()\n",
    "\n",
    "        self.cov = nn.Conv2d(in_channel, out_channel, 3, 1, 1)\n",
    "\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.nor = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cov(x)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.nor(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class dou_cov(nn.Module):\n",
    "    \"\"\"双层卷积网络\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, dropout=False, normalize=False):\n",
    "        super(dou_cov, self).__init__()\n",
    "        self.cov1 = nor_cov(in_channel=channel, out_channel=channel, dropout=dropout, normalize=normalize)\n",
    "        self.cov2 = nor_cov(in_channel=channel, out_channel=channel, dropout=dropout, normalize=normalize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cov1(x)\n",
    "        x = self.cov2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNN_net(nn.Module):\n",
    "    \"\"\"自定义卷积网络\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_net, self).__init__()\n",
    "\n",
    "        self.cov1 = nor_cov(in_channel=3, out_channel=64, dropout=True, normalize=True)\n",
    "        # self.dou_cov1 = dou_cov(channel=64, dropout=True, normalize=True)\n",
    "        # self.dou_cov2 = dou_cov(channel=64, dropout=True, normalize=True)\n",
    "        self.res1 = residual_block(64, dropout=False)\n",
    "        self.res2 = residual_block(64, dropout=False)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.cov2 = nor_cov(in_channel=64, out_channel=128, dropout=True, normalize=True)\n",
    "        # self.dou_cov3 = dou_cov(channel=128, dropout=True, normalize=True)\n",
    "        # self.dou_cov4 = dou_cov(channel=128, dropout=True, normalize=True)\n",
    "        self.res3 = residual_block(128, dropout=False)\n",
    "        self.res4 = residual_block(128, dropout=False)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.cov3 = nor_cov(in_channel=128, out_channel=256, dropout=True, normalize=True)\n",
    "        # self.dou_cov5 = dou_cov(channel=256, dropout=True, normalize=True)\n",
    "        # self.dou_cov6 = dou_cov(channel=256, dropout=True, normalize=True)\n",
    "        self.res5 = residual_block(256, dropout=False)\n",
    "        self.res6 = residual_block(256, dropout=False)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.cov4 = nor_cov(in_channel=256, out_channel=512, dropout=True, normalize=True)\n",
    "        # self.dou_cov7 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        # self.dou_cov8 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        self.res7 = residual_block(512, dropout=False)\n",
    "        self.res8 = residual_block(512, dropout=False)\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.cov5 = nor_cov(in_channel=512, out_channel=256, dropout=True, normalize=True)\n",
    "        # self.dou_cov7 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        # self.dou_cov8 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        self.res9 = residual_block(256, dropout=False)\n",
    "        self.res10 = residual_block(256, dropout=False)\n",
    "\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.cov6 = nor_cov(in_channel=256, out_channel=128, dropout=True, normalize=True)\n",
    "        # self.dou_cov7 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        # self.dou_cov8 = dou_cov(channel=512, dropout=True, normalize=True)\n",
    "        self.res11 = residual_block(128, dropout=False)\n",
    "        self.res12 = residual_block(128, dropout=True)\n",
    "\n",
    "        self.pool6 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 1 * 1, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cov1(x)\n",
    "        # x = self.dou_cov1(x)\n",
    "        # x = self.dou_cov2(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.cov2(x)\n",
    "        # x = self.dou_cov3(x)\n",
    "        # x = self.dou_cov4(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.cov3(x)\n",
    "        # x = self.dou_cov5(x)\n",
    "        # x = self.dou_cov6(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.res6(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.cov4(x)\n",
    "        # x = self.dou_cov7(x)\n",
    "        # x = self.dou_cov8(x)\n",
    "        x = self.res7(x)\n",
    "        x = self.res8(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.cov5(x)\n",
    "        # x = self.dou_cov5(x)\n",
    "        # x = self.dou_cov6(x)\n",
    "        x = self.res9(x)\n",
    "        x = self.res10(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        x = self.cov6(x)\n",
    "        # x = self.dou_cov7(x)\n",
    "        # x = self.dou_cov8(x)\n",
    "        x = self.res11(x)\n",
    "        x = self.res12(x)\n",
    "        x = self.pool6(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lab2Model(object):\n",
    "\n",
    "    def __init__(self, batch_size=64, num_workers=10, seed=0):\n",
    "        self.seed = seed\n",
    "        self.setup_seed()\n",
    "        # 这里 ToTensor 会把 numpy 类型转换为 tensor 类型，并对数据归一化到 [0, 1]\n",
    "        train_dataset = Data(type_=\"train\", transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "        # 从训练数据中手动划分训练集和验证集\n",
    "        self.train_dataset, self.val_dataset = random_split(train_dataset, \n",
    "                                                            [int(len(train_dataset) * 0.8), len(train_dataset) - int(len(train_dataset) * 0.8)],\n",
    "                                                            generator=torch.Generator().manual_seed(0))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_dataloader = DataLoader(dataset=self.train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=num_workers,\n",
    "                                           drop_last=True)\n",
    "        self.val_dataloader = DataLoader(dataset=self.val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=num_workers,\n",
    "                                         drop_last=True)\n",
    "\n",
    "        self.net = None\n",
    "        self.lr = None\n",
    "        self.optimizer = None\n",
    "        self.device = None\n",
    "        self.schedule = None\n",
    "        self.fig_name = None\n",
    "        self.loss_list = {\"train\": [], \"val\": []}\n",
    "\n",
    "    def train(self, lr=0.01, epochs=10, device=\"cuda\", wait=8, lrd=False, fig_name=\"lab1\"):\n",
    "        self.device = torch.device(device) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.lr = lr\n",
    "        self.fig_name = fig_name\n",
    "        self.net = CNN_net().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        if lrd:\n",
    "            self.schedule = ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "        total_params = sum([param.nelement() for param in self.net.parameters() if param.requires_grad])\n",
    "\n",
    "        print(\">>> Total params: {}\".format(total_params))\n",
    "\n",
    "        print(\">>> Start training\")\n",
    "        min_val_loss = np.inf\n",
    "        min_val_loss_acc = 0.0\n",
    "        delay = 0\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # train train data\n",
    "            for data in tqdm(self.train_dataloader):\n",
    "                labels, inputs = data\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.net(inputs)\n",
    "                loss = nn.CrossEntropyLoss()(output=outputs, target=labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # calc train loss and train acc\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in self.train_dataloader:\n",
    "                    labels, inputs = data\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    outputs = self.net(inputs)\n",
    "                    loss = nn.CrossEntropyLoss()(output=outputs, target=labels)\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                train_loss = train_loss / len(self.train_dataloader)\n",
    "                self.loss_list['train'].append(train_loss)\n",
    "\n",
    "                for data in self.val_dataloader:\n",
    "                    labels, inputs = data\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    outputs = self.net(inputs)\n",
    "                    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_acc += self.acc(labels=labels.cpu().numpy(), outputs=outputs.detach().cpu().numpy())\n",
    "                val_loss = val_loss / len(self.val_dataloader)\n",
    "                val_acc = val_acc / len(self.val_dataloader)\n",
    "                self.loss_list['val'].append(val_loss)\n",
    "                print(f\"Epoch {epoch}: train loss {train_loss:10.6f}, \"\n",
    "                      f\"val loss {val_loss:10.6f}\")\n",
    "            # if necessary, reduce the learning rate by val loss\n",
    "            if lrd:\n",
    "                self.schedule.step(val_loss)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                min_val_loss_acc = val_acc\n",
    "                print(f\"Update min_val_loss to {min_val_loss:10.6f}\")\n",
    "                delay = 0\n",
    "            else:\n",
    "                delay = delay + 1\n",
    "\n",
    "            if delay > wait:\n",
    "                break\n",
    "        print(\">>> Finished training\")\n",
    "        self.plot_loss()\n",
    "        print(\">>> Finished plot loss\")\n",
    "        return min_val_loss_acc\n",
    "\n",
    "    def test(self):\n",
    "        test_data = Data(type_=\"val\", transform=transforms.Compose([transforms.ToTensor()]))\n",
    "        test_data_loader = DataLoader(dataset=test_data,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=self.num_workers,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        test_acc = 0.0\n",
    "        for data in test_data_loader:\n",
    "            labels, inputs = data\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.net(inputs)\n",
    "            test_acc += self.acc(labels.numpy(), outputs.detach().cpu().numpy())\n",
    "            \n",
    "        test_acc = test_acc / len(test_data_loader)\n",
    "        return test_acc\n",
    "\n",
    "    def acc(self, labels, outputs, type_=\"top1\"):\n",
    "        acc = 0\n",
    "        if type_ == \"top1\":\n",
    "            pre_labels = np.argmax(outputs, axis=1)\n",
    "            labels = labels.reshape(len(labels))\n",
    "            acc = np.sum(pre_labels == labels) / len(pre_labels)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def setup_seed(self):\n",
    "        seed = self.seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure()\n",
    "        train_loss = self.loss_list['train']\n",
    "        val_loss = self.loss_list['val']\n",
    "        plt.plot(train_loss, c=\"red\", label=\"train_loss\")\n",
    "        plt.plot(val_loss, c=\"blue\", label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"CrossEntropyLoss\")\n",
    "        plt.title(\"CrossEntropyLoss of Train and Validation in each Epoch\")\n",
    "        plt.savefig(f\"../fig/{self.fig_name}_loss.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始初始化模型和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Total params: 18553416\n",
      ">>> Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = Lab2Model(batch_size=64, num_workers=8, seed=0)\n",
    "\n",
    "# 最好还是使用 GPU, CPU 太慢了。。。\n",
    "model.train(lr=0.001, epochs=80, device='cpu', wait=12, lrd=True, fig_name='test')\n",
    "\n",
    "# 选择好超参数后，测试模型表现\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
